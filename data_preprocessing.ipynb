{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE : https://www.kaggle.com/datasets/msambare/fer2013/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## Preprocessing in CNNs one should always remember considering:\n",
    "1. Normalization: Scaling pixel values to a range (often [0, 1]) helps the neural network converge faster during training and improves numerical stability.\n",
    "2. Resizing: Images in a dataset may vary in dimensions, so resizing them to a uniform size ensures consistent input dimensions for the model.\n",
    "3. Grayscale Conversion: Converting RGB images to grayscale reduces computational complexity and focuses the model on relevant features for grayscale tasks like facial expression recognition.\n",
    "4. Techniques like rotation, flipping, or cropping can artificially increase the diversity of the training data, helping the model generalize better.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## What I did in this code:\n",
    "1. Grayscale Conversion: \n",
    "- Images were loaded and resized as grayscale using load_img with color_mode=\"grayscale\".\n",
    "- I did this so as to learn features in facial expressions without the complexity of color.\n",
    "- Also grayscale conversion simplifies the data, focusing on facial structure and expression rather than color variations or noise.\n",
    "\n",
    "2. Normalization:\n",
    "- Pixel values were scaled to [0, 1] by dividing by 255.0 after converting images to numpy arrays.\n",
    "- This will help to stabilize and accelerate model training by ensuring consistent data range across all pixels.\n",
    "- Normalization prepares the data to be within a range that is easier for the model to process, enhancing training efficiency .\n",
    "\n",
    "3. Visualization:\n",
    "- Displayed preprocessed images to ensure correctness and verified shapes (number of samples, height, width) for both training and testing datasets.\n",
    "- This helps to ensures that data is correctly prepared and aligned with model expectations before training begins.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Objective: The goal of the following code snippet is to visualize a sample of images from the training dataset for facial expression recognition.\n",
    "* Folder Structure: The images are organized into subfolders where each subfolder represents a different facial expression (e.g., \"happy\", \"sad\", etc.).\n",
    "\n",
    "* Steps in the Code: \n",
    "1. Iteration through Directories: We use 'os.listdir' to iterate through each subfolder (expression) in the train directory (base_path + \"train/\").\n",
    "2. Loading and Displaying Images: For each expression, we load up to 5 images (for i in range(1, 6)). We construct the path to each image using os.path.join and os.listdir. Then, we load each image using load_img from Keras, resize it to a standard size (pic_size x pic_size), and display it using Matplotlib (plt.imshow).\n",
    "3. Labeling: We label each image plot with its corresponding expression (plt.xlabel(expression)).\n",
    "\n",
    "* Purpose of Visualization: Visualizing a sample of the dataset helps us:\n",
    "- Verify that images are loaded correctly.\n",
    "- Understand the structure and organization of the dataset.\n",
    "- Ensure that images and labels match as expected.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/Documents/GitHub/GitHub/Face-expression-recognition-with-Deep-Learning/archive (3)/\"\n",
    "\n",
    "plt.figure(0, figsize=(12,20))    #plt.figure : it is used to create a new figure # 0: figure number, figsize: width, height in inches\n",
    "counter = 0 # Counter to keep track of number of images displayed\n",
    "\n",
    "# Function to preprocess images: \n",
    "def preprocess_image(img_path):          #img_path: path to the image   and      #preprocess_image function : to load, resize and normalize the image\n",
    "    img = load_img(img_path, target_size=(pic_size, pic_size))  # Load image and resize\n",
    "    img_array = img_to_array(img)  # Convert image to numpy array\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Lists to store preprocessed images and their labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through each expression folder in the train directory\n",
    "for expression in os.listdir(base_path + \"train/\"):\n",
    "    if not os.path.isdir(os.path.join(base_path + \"train/\", expression)):\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        counter += 1\n",
    "        plt.subplot(7, 5, counter)\n",
    "        \n",
    "        # Load, preprocess, and display the image\n",
    "        img_name = os.listdir(os.path.join(base_path, \"train\", expression))[i]\n",
    "        img_path = os.path.join(base_path, \"train\", expression, img_name)\n",
    "        img_array = preprocess_image(img_path)\n",
    "        \n",
    "        plt.imshow(img_array, cmap=\"gray\")  # Display the preprocessed image\n",
    "        plt.xlabel(expression)  # Show expression label as xlabel\n",
    "\n",
    "        # Append preprocessed image and its label to lists\n",
    "        images.append(img_array)\n",
    "        labels.append(expression)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### Code Explanation:\n",
    "1. Preprocessing Function (preprocess_image):\n",
    "- preprocess_image function : to encapsulate the preprocessing steps for each image.\n",
    "- It loads an image from img_path, resizes it to (pic_size, pic_size) using load_img from Keras.\n",
    "- Converts the image to a numpy array (img_to_array).\n",
    "- Normalizes the pixel values by dividing by 255.0, ensuring all values are between 0 and 1.\n",
    "\n",
    "2. Integration:\n",
    "- Inside the nested loops, instead of loading and displaying raw images, we now call preprocess_image to obtain the preprocessed image array (img_array).\n",
    "- plt.imshow(img_array, cmap=\"gray\") displays the preprocessed image using Matplotlib.\n",
    "\n",
    "3. Visualization:\n",
    "- The code continues to iterate through each expression folder in the train directory, displaying up to 5 preprocessed images per expression.\n",
    "- Each image plot is labeled with its corresponding expression (plt.xlabel(expression)).\n",
    "\n",
    "* Lists for Storage: We've introduced two lists, images and labels, to store the preprocessed images and their corresponding labels.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (28, 48, 48, 3)\n",
      "Train labels shape: (28,)\n",
      "Test images shape: (7, 48, 48, 3)\n",
      "Test labels shape: (7,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import train_test_split function\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### Explanation:\n",
    "- Data Splitting: After preprocessing, we convert images and labels lists into numpy arrays (np.array(images) and np.array(labels)). Then, we split these arrays into training (train_images, train_labels) and testing (test_images, test_labels) sets using train_test_split from sklearn.model_selection.\n",
    "- Visualization and Preprocessing: The code continues to visualize a sample of preprocessed images while appending them to images and labels lists. Each image plot is labeled with its corresponding expression.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### It seems there was an issue with the load_img function. \n",
    "- Error Issue: When loading images using load_img function, the argument grayscale=True was causing a TypeError.\n",
    "- So this prevented proper loading and preprocessing of images in grayscale format.\n",
    "- So I fixed by changing grayscale=True to color_mode=\"grayscale\" in the load_img function.\n",
    "- This ensured that images were correctly loaded and resized as grayscale, aligning with the expected format for facial expression recognition tasks.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's rerun the same above code with few corrections : Let's look at the o/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split function\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/Documents/GitHub/GitHub/Face-expression-recognition-with-Deep-Learning/archive (3)/\"\n",
    "\n",
    "plt.figure(0, figsize=(12,20))\n",
    "cpt = 0\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path):  #PIL : Python Imaging Library\n",
    "    img = load_img(img_path, color_mode=\"grayscale\", target_size=(pic_size, pic_size))  # Load image as grayscale and resize\n",
    "    img_array = img_to_array(img)  # Convert image to numpy array\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Lists to store preprocessed images and their labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through each expression folder in the train directory\n",
    "for expression in os.listdir(base_path + \"train/\"):\n",
    "    if not os.path.isdir(os.path.join(base_path + \"train/\", expression)):\n",
    "        continue\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        cpt += 1\n",
    "        plt.subplot(7, 5, cpt)\n",
    "        \n",
    "        # Load, preprocess, and display the image\n",
    "        img_name = os.listdir(os.path.join(base_path, \"train\", expression))[i]\n",
    "        img_path = os.path.join(base_path, \"train\", expression, img_name)\n",
    "        img_array = preprocess_image(img_path)\n",
    "        \n",
    "        plt.imshow(img_array.reshape(pic_size, pic_size), cmap=\"gray\")  # Display the preprocessed image\n",
    "        plt.xlabel(expression)  # Show expression label as xlabel\n",
    "        \n",
    "        # Append preprocessed image and its label to lists\n",
    "        images.append(img_array)\n",
    "        labels.append(expression)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now the corrected o/p is:\n",
    "Train images shape: (28, 48, 48, 1)\n",
    "Train labels shape: (28,)\n",
    "Test images shape: (7, 48, 48, 1)\n",
    "Test labels shape: (7,)\n",
    "\n",
    "\n",
    "### Output Interpretation: \n",
    "\n",
    "1. Train images shape: (28, 48, 48, 1)\n",
    "- 28 samples for training\n",
    "- Each image is 48x48 pixels\n",
    "- Grayscale images with 1 channel (since they are loaded in grayscale)\n",
    "\n",
    "2. Train labels shape: (28,)\n",
    "- Corresponding labels for the training images\n",
    "\n",
    "3. Test images shape: (7, 48, 48, 1)\n",
    "- 7 samples for testing\n",
    "- Each image is 48x48 pixels\n",
    "- Grayscale images with 1 channel\n",
    "\n",
    "4. Test labels shape: (7,)\n",
    "- Corresponding labels for the testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
