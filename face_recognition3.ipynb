{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports: These lines import necessary libraries:\n",
    "* numpy : numerical operations.\n",
    "* seaborn : statistical data visualization.\n",
    "* load_img & img_to_array from keras.preprocessing.image : loading and processing images.\n",
    "* matplotlib.pyplot as plt : plotting images.\n",
    "* os : interacting with the operating system to handle file paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images for every different expression\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the image: 48*48 pixels\n",
    "pic_size = 48                            #Specifies the size of each image (48x48 pixels in this case)\n",
    "\n",
    "# input path for the images\n",
    "base_path = \"/Users/pratiksha/Downloads/Face_expression/images/\"\n",
    "\n",
    "plt.figure(0, figsize=(12,20)) # Specifies the size of the plot\n",
    "cpt = 0                        # Used to count the number of images displayed -- Counter variable\n",
    "\n",
    "\n",
    "for expression in os.listdir(base_path + \"train/\"):                    # Iterates through each expression folder (angry, disgust, etc.) within the train directory.\n",
    "    expression_path = os.path.join(base_path, \"train\", expression)\n",
    "    if not os.path.isdir(expression_path):\n",
    "        continue  # Skip if it's not a directory\n",
    "\n",
    "    for i, img_file in enumerate(os.listdir(expression_path)[:5]):     # Loops through the first 5 images (i ranges from 1 to 5) within each expression folder.\n",
    "        cpt = cpt + 1                                                  # Increments the counter variable\n",
    "        plt.subplot(7, 5, cpt)                                         # Creates a subplot grid with 7 rows and 5 columns, placing the current image in position cpt.\n",
    "        img_path = os.path.join(expression_path, img_file)\n",
    "        img = load_img(img_path, target_size=(pic_size, pic_size))\n",
    "        plt.imshow(img, cmap=\"gray\")                                   # Displays the image with a grayscale colormap.\n",
    "\n",
    "plt.tight_layout()  \n",
    "\n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identifying facial expressions from images is challenging for algorithms due to factors like :\n",
    "* the images have a low resolution\n",
    "* the faces are not in the same position\n",
    "* some images have text written on them\n",
    "* some people hide part of their faces with their hands\n",
    "- However all this diversity of images will contribute to make a more generalizable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164 happy images\n",
      "4938 sad images\n",
      "4103 fear images\n",
      "3205 surprise images\n",
      "4982 neutral images\n",
      "3993 angry images\n",
      "436 disgust images\n"
     ]
    }
   ],
   "source": [
    "for expression in os.listdir(os.path.join(base_path, \"train\")):        \n",
    "         #Iterates through each expression folder (angry, disgust, etc.) within the train directory\n",
    "\n",
    "    expression_path = os.path.join(base_path, \"train\", expression)\n",
    "    if not os.path.isdir(expression_path):                     \n",
    "        continue  # Skip non-directory files like .DS_Store\n",
    "\n",
    "    # Count and print the number of images in each expression folder\n",
    "    num_images = len(os.listdir(expression_path))      # Counts the number of images in the current expression folder\n",
    "    print(f\"{num_images} {expression} images\")         # Prints the number of images in the current expression folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The image expressions in our training dataset are pretty balanced, except for the 'disgust' category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the data generators : Setting up data generators in Keras using ImageDataGenerator allows efficient loading and preprocessing of image data for training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Defining base path\n",
    "base_path = \"/Users/pratiksha/Downloads/Face_expression/\"\n",
    "\n",
    "\n",
    "# number of images to feed into the NN for every batch\n",
    "batch_size = 128                  #Specifies the number of images to feed into the neural network for every batch during training and validation.\n",
    "\n",
    "datagen_train = ImageDataGenerator()                \n",
    "datagen_validation = ImageDataGenerator()\n",
    "\n",
    "# Creates instances of ImageDataGenerator for training (datagen_train) and validation (datagen_validation). \n",
    "# These generators will handle data augmentation, scaling, and other preprocessing tasks.\n",
    "\n",
    "train_generator = datagen_train.flow_from_directory(base_path + \"train\",            # flow_from_directory: Generates batches of augmented/normalized data from image files in the train directory.\n",
    "                                                    target_size=(pic_size,pic_size), # Resizes images to (pic_size, pic_size) pixels.\n",
    "                                                    color_mode=\"grayscale\",           # Converts images to grayscale format.\n",
    "                                                    batch_size=batch_size,            # Number of images per batch to be yielded from the generator.\n",
    "                                                    class_mode='categorical',         # Returns one-hot encoded labels for multi-class classification.\n",
    "                                                    shuffle=True)                     #  Shuffles the order of images after every epoch.\n",
    "\n",
    "validation_generator = datagen_validation.flow_from_directory(base_path + \"validation\", # fetches batches of validation data from the validation directory.\n",
    "                                                    target_size=(pic_size,pic_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning models are trained by being fed with batches of data. Keras has a very useful class to automatically feed data from a directory: ImageDataGenerator.\n",
    "\n",
    "- It can also perform data augmentation while getting the images (randomly rotating the image, zooming, etc.). This method is often used as a way to artificially get more data when the dataset has a small size.\n",
    "\n",
    "- The function flow_from_directory() specifies how the generator should import the images (path, image size, colors, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Found 28821 images belonging to 7 classes: indicates that in the training dataset (base_path + \"train\"), there are a total of 28,821 images distributed among 7 classes.\n",
    "* Found 7066 images belonging to 7 classes: Similarly, in your validation dataset (base_path + \"validation\"), there are 7,066 images also distributed among the same 7 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial Image Checking: Earlier, we used a loop to display a subset of images (specifically, the first 5 images) from each expression category (angry, disgust, fear, happy, neutral, sad, surprise) in the training data. This was likely done to visually inspect the data and ensure that images are loaded correctly and represent different facial expressions.\n",
    "\n",
    "2. Setting up Data Generators: After verifying the data, we proceeded to set up 'ImageDataGenerator' instances for both training and validation datasets. These generators will be used to feed batches of images into your neural network during the training process.\n",
    "\n",
    "3. Batch Size: We specified a batch_size of 128 when setting up the train_generator and validation_generator. This means that during training, the neural network will process 128 images at a time (or as many as can fit into memory for the hardware configuration). This batch size is a common parameter in deep learning training and affects how many images are processed before updating the model's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTIONS:\n",
    "1. USE of 'train_generator' & 'validation_generator' : \n",
    "- Instances of ImageDataGenerator from Keras that handle data preprocessing and augmentation (if specified). \n",
    "- They generate batches of images and their corresponding labels directly from directories of images, allowing you to work with large datasets that don't fit into memory.\n",
    "- 'train_generator' : used during the training phase of your neural network. It provides batches of images to the model, allowing it to update its weights based on the gradients computed from these batches.\n",
    "- validation_generator : used to evaluate the model's performance on a separate set of data that the model hasn't seen during training. It helps monitor the model's generalization ability and prevents overfitting.\n",
    "\n",
    "2. BATCH SIZE:\n",
    "- Batch size refers to the number of samples (images, in this case) that the model processes at a time before updating the weights.\n",
    "When you set batch_size=128, the model will process 128 images in each iteration (batch) during training.\n",
    "Benefits:\n",
    "\n",
    "* Efficiency: Processing data in batches is more memory efficient. Instead of loading all images into memory at once, which may not be feasible for large datasets, batches allow you to work with manageable chunks.\n",
    "* Gradient Descent: Batch processing allows for more stable gradient descent updates. It computes gradients based on the average loss across the batch, providing smoother convergence towards the optimal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratiksha/.pyenv/versions/3.9.19/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# number of possible label values\n",
    "nb_classes = 7\n",
    "\n",
    "# Initialising the CNN\n",
    "model = Sequential()  # initializes an empty sequential model.  \n",
    "\n",
    "# 1 - Convolution \n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))         #\n",
    "# #Each model.add() call adds a layer to the network in sequence and Conv2D: Adds a 2D convolutional layer\n",
    "model.add(BatchNormalization())                                              #BatchNormalization: Adds batch normalization to stabilize and speed up training.\n",
    "model.add(Activation('relu'))                                                #Adds an activation function (ReLU in this case) to introduce non-linearity.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))                                    #MaxPooling2D: Adds a max-pooling layer to down-sample the input.\n",
    "model.add(Dropout(0.25))                                                     #Dropout: Adds a dropout layer for regularization.\n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0001)                                                                 #optimizer: Specifies the optimizer (Adam in this case) to use for training.\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])   #metrics: Specifies the metrics to evaluate the model (accuracy in this case).\n",
    "     #loss: Specifies the loss function (categorical crossentropy for multi-class classification).\n",
    "     #categorical_crossentropy : Computes the cross-entropy loss between true labels and predicted labels.\n",
    "     #crossentropy : measures the performance of a classification model whose output is a probability value between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratiksha/.pyenv/versions/3.9.19/lib/python3.9/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1847 - loss: 2.2413\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31889, saving model to model_weights.keras\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m726s\u001b[0m 3s/step - accuracy: 0.1849 - loss: 2.2405 - val_accuracy: 0.3189 - val_loss: 1.7713\n",
      "Epoch 2/50\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:06\u001b[0m 4s/step - accuracy: 0.3281 - loss: 1.8819"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 16:32:21.198453: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Users/pratiksha/.pyenv/versions/3.9.19/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n",
      "2024-07-26 16:32:21.377599: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.31889\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.3281 - loss: 1.8819 - val_accuracy: 0.0000e+00 - val_loss: 2.6517\n",
      "Epoch 3/50\n",
      "\u001b[1m122/225\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4:53\u001b[0m 3s/step - accuracy: 0.2734 - loss: 1.8858"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This calculates the time taken to execute the cell. It's useful for measuring performance.\n",
    "\n",
    "epochs = 50\n",
    "# Specifies the number of epochs to train the model. Each epoch means one complete pass through the entire training dataset.\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Imports the ModelCheckpoint class from keras.callbacks. This is used to save the model at certain points during training.\n",
    "\n",
    "# Define the checkpoint to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"model_weights.keras\",  # The file path where the model weights will be saved. Updated to end with .keras for clarity.\n",
    "    monitor='val_accuracy',  # Metric to be monitored. 'val_accuracy' means validation accuracy.\n",
    "    verbose=1,  # Verbosity mode. 1 means that messages will be printed when the model is being saved.\n",
    "    save_best_only=True,  # If True, the latest best model according to the monitored metric will not be overwritten.\n",
    "    mode='max'  # Mode for the monitored metric. 'max' means that the model will be saved when the quantity monitored has stopped increasing.\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "# A list of callbacks to pass to the model during training. Here, it contains only the checkpoint callback.\n",
    "\n",
    "# Train the model using model.fit instead of model.fit_generator\n",
    "history = model.fit(\n",
    "    x=train_generator,  # The training data. Here, train_generator is used to provide batches of data during training.\n",
    "    epochs=epochs,  # The number of epochs to train the model.\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,  \n",
    "    # The number of steps per epoch. This is the total number of samples in the training data divided by the batch size.\n",
    "    \n",
    "    validation_data=validation_generator,  # The validation data. Here, validation_generator is used to provide batches of validation data.\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size,  \n",
    "    # The number of validation steps per epoch. This is the total number of samples in the validation data divided by the batch size.\n",
    "    \n",
    "    callbacks=callbacks_list  # List of callbacks to apply during training. Here, it includes the checkpoint callback to save the best model.\n",
    ")\n",
    "\n",
    "# Explanation for validation_generator.n // validation_generator.batch_size:\n",
    "# This calculates the number of batches (steps) needed to go through the entire validation dataset once.\n",
    "# - validation_generator.n is the total number of samples in the validation dataset.\n",
    "# - validation_generator.batch_size is the number of samples in each batch.\n",
    "# The integer division (//) ensures we get the whole number of batches. For example, if there are 1000 validation samples and the batch size is 32,\n",
    "# this would be 1000 // 32 = 31 steps per epoch for validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Size: This is the number of samples processed before the model's internal parameters are updated.\n",
    "# We defined a batch size of 128.\n",
    "\n",
    "# Total Batches: This is the total number of times the model updates its parameters in one epoch.\n",
    "# The total number of batches is calculated as follows:\n",
    "# Total Batches = Total Number of Samples / Batch Size\n",
    "\n",
    "# If our dataset has 28,800 images and we have a batch size of 128, the total number of batches would be:\n",
    "# Total Batches = 28,800 / 128 ≈ 225\n",
    "\n",
    "# Thus, the total number of batches (225) is correct for a dataset of 28,800 images with a batch size of 128.\n",
    "# This is why you see the progress output 34/225.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Structure and Weights to JSON:\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer: Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix of our predictions\n",
    "\n",
    "# Compute predictions\n",
    "predictions = model.predict(validation_generator)\n",
    "y_pred = [np.argmax(probas) for probas in predictions]\n",
    "y_test = validation_generator.classes\n",
    "class_names = list(validation_generator.class_indices.keys())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized Confusion Matrix:\n",
    "[[0.48 0.05 0.09 0.06 0.10 0.17 0.04]   Class 0 angry\n",
    " [0.14 0.63 0.08 0.03 0.06 0.05 0.02]   Class 1 disgust\n",
    " [0.11 0.02 0.33 0.05 0.08 0.25 0.16]   Class 2 fear\n",
    " [0.02 0.00 0.02 0.84 0.04 0.05 0.03]   Class 3 happy\n",
    " [0.11 0.01 0.05 0.11 0.51 0.19 0.03]   Class 4 neutral\n",
    " [0.10 0.02 0.09 0.07 0.13 0.56 0.02]   Class 5 sad\n",
    " [0.02 0.00 0.07 0.05 0.03 0.03 0.79]]  Class 6 surprise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 0 (angry) has 48% correctly predicted, with notable confusion in Class 5 (sad) (17%).\n",
    "Class 1 (disgust) has 63% correctly predicted, with some confusion in Class 0 (angry) (14%) and Class 2 (fear) (8%).\n",
    "Class 2 (fear) has 33% correctly predicted, with significant confusion in Class 5 (sad) (25%) and Class 6 (surprise) (16%).\n",
    "Class 3 (happy) has 84% correctly predicted, with some confusion in Class 0 (angry) (2%) and Class 4 (neutral) (4%).\n",
    "Class 4 (neutral) has 51% correctly predicted, with notable confusion in Class 0 (angry) (11%) and Class 5 (sad) (19%).\n",
    "Class 5 (sad) has 56% correctly predicted, with significant confusion in Class 4 (neutral) (13%) and Class 2 (fear) (9%).\n",
    "Class 6 (surprise) has 79% correctly predicted, with some confusion in Class 2 (fear) (7%) and Class 0 (angry) (2%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Our model demonstrates strong predictive capabilities for happy and surprised facial expressions, achieving high accuracies of 84% and 79% respectively. However, it faces challenges in accurately distinguishing fear from sadness and neutral expressions from other emotions. Despite these complexities, the model represents a significant advancement compared to existing benchmarks in emotion recognition. Further research and optimization efforts could enhance its performance, making it more robust for real-world applications.\n",
    "\n",
    "If you have any more questions or if there's anything else you'd like to explore, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
