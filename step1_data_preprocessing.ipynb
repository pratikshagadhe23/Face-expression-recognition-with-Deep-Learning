{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCE : https://www.kaggle.com/datasets/msambare/fer2013/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## Preprocessing in CNNs one should always remember considering:\n",
    "1. Normalization: Scaling pixel values to a range (often [0, 1]) helps the neural network converge faster during training and improves numerical stability.\n",
    "2. Resizing: Images in a dataset may vary in dimensions, so resizing them to a uniform size ensures consistent input dimensions for the model.\n",
    "3. Grayscale Conversion: Converting RGB images to grayscale reduces computational complexity and focuses the model on relevant features for grayscale tasks like facial expression recognition.\n",
    "4. Techniques like rotation, flipping, or cropping can artificially increase the diversity of the training data, helping the model generalize better.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## What I did in this code:\n",
    "1. Grayscale Conversion: \n",
    "- Images were loaded and resized as grayscale using load_img with color_mode=\"grayscale\".\n",
    "- I did this so as to learn features in facial expressions without the complexity of color.\n",
    "- Also grayscale conversion simplifies the data, focusing on facial structure and expression rather than color variations or noise.\n",
    "\n",
    "2. Normalization:\n",
    "- Pixel values were scaled to [0, 1] by dividing by 255.0 after converting images to numpy arrays.\n",
    "- This will help to stabilize and accelerate model training by ensuring consistent data range across all pixels.\n",
    "- Normalization prepares the data to be within a range that is easier for the model to process, enhancing training efficiency .\n",
    "\n",
    "3. Visualization:\n",
    "- Displayed preprocessed images to ensure correctness and verified shapes (number of samples, height, width) for both training and testing datasets.\n",
    "- This helps to ensures that data is correctly prepared and aligned with model expectations before training begins.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Objective: The goal of the following code snippet is to visualize a sample of images from the training dataset for facial expression recognition.\n",
    "* Folder Structure: The images are organized into subfolders where each subfolder represents a different facial expression (e.g., \"happy\", \"sad\", etc.).\n",
    "\n",
    "* Steps in the Code: \n",
    "1. Iteration through Directories: We use 'os.listdir' to iterate through each subfolder (expression) in the train directory (base_path + \"train/\").\n",
    "2. Loading and Displaying Images: For each expression, we load up to 5 images (for i in range(1, 6)). We construct the path to each image using os.path.join and os.listdir. Then, we load each image using load_img from Keras, resize it to a standard size (pic_size x pic_size), and display it using Matplotlib (plt.imshow).\n",
    "3. Labeling: We label each image plot with its corresponding expression (plt.xlabel(expression)).\n",
    "\n",
    "* Purpose of Visualization: Visualizing a sample of the dataset helps us:\n",
    "- Verify that images are loaded correctly.\n",
    "- Understand the structure and organization of the dataset.\n",
    "- Ensure that images and labels match as expected.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports: These lines import necessary libraries:\n",
    "* numpy : numerical operations.\n",
    "* seaborn : statistical data visualization.\n",
    "* load_img & img_to_array from keras.preprocessing.image : loading and processing images.\n",
    "* matplotlib.pyplot as plt : plotting images.\n",
    "* os : interacting with the operating system to handle file paths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline Steps:\n",
    "a. Image Loading: Load the image files from the dataset, converting them to a format suitable for neural networks (e.g., NumPy arrays).\n",
    "b. Resizing: Resize all images to a consistent size (e.g., 48x48 pixels), which is a standard size for many facial expression recognition models.\n",
    "c. Grayscale Conversion: Convert images to grayscale to reduce computational complexity, especially if you're using convolutional neural networks (CNNs), which can perform \n",
    "                         better with grayscale images in some tasks like facial recognition.\n",
    "d. Normalization: Normalize the pixel values to a range between 0 and 1 by dividing the pixel values by 255. This helps the model converge more quickly during training.\n",
    "e. Augmentation: Data augmentation involves creating new training images through random transformations (e.g., rotation, flipping, zoom, shifts) to artificially expand the dataset \n",
    "                         and introduce variability.Augmentation helps combat overfitting, especially when working with a small or imbalanced dataset.\n",
    "f. One-Hot Encoding: Convert labels into one-hot encoded vectors to match the expected format for classification tasks in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train set: To train the model.\n",
    "* Validation set: To tune hyperparameters and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48                            #Specifies the size of each image (48x48 pixels in this case)\n",
    "\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/GitHub/Face-expression-recognition-with-Deep-Learning/images\"\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path, target_size=(48, 48)):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = load_img(img_path, target_size=target_size, color_mode=\"grayscale\")  # Grayscale for custom CNNs\n",
    "    img_array = img_to_array(img)  # Convert to NumPy array\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_dataset(base_path, subset='train'):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        for img_name in os.listdir(expression_path):\n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            try:\n",
    "                img_array = preprocess_image(img_path)\n",
    "                images.append(img_array)\n",
    "                labels.append(label_map[expression])  # Use label_map safely\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize sample images\n",
    "def visualize_samples(base_path, subset='train'):\n",
    "    \"\"\"Visualize sample images for each expression.\"\"\"\n",
    "    plt.figure(figsize=(12, 20))           # Specifies the size of the plot\n",
    "    counter = 0                            # Used to count the number of images displayed -- Counter variable\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    \n",
    "    for expression in os.listdir(subset_path):                      # Iterates through each expression folder (angry, disgust, etc.) within the train directory.\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue                                                # Skip if it's not a directory\n",
    "        \n",
    "        for i, img_name in enumerate(os.listdir(expression_path)[:5]):  # Loops through the first 5 images (i ranges from 1 to 5) within each expression folder.\n",
    "            counter += 1                                                # Increments the counter variable\n",
    "            plt.subplot(7, 5, counter)                                  # Creates a subplot grid with 7 rows and 5 columns, placing the current image in position cpt.\n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            img_array = preprocess_image(img_path)\n",
    "            plt.imshow(img_array, cmap=\"gray\")                          # Displays the image with a grayscale colormap.\n",
    "            plt.xlabel(expression)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to count images per expression\n",
    "def count_images_per_expression(base_path, subset='train'):\n",
    "    \"\"\"Count and display the number of images per expression.\"\"\"\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue\n",
    "        \n",
    "        num_images = len(os.listdir(expression_path))\n",
    "        print(f\"{num_images} {expression} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164 happy images\n",
      "4938 sad images\n",
      "4103 fear images\n",
      "3205 surprise images\n",
      "4982 neutral images\n",
      "3993 angry images\n",
      "436 disgust images\n"
     ]
    }
   ],
   "source": [
    "count_images_per_expression(base_path, subset='train')\n",
    "##visualize_samples(base_path, subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825 happy images\n",
      "1139 sad images\n",
      "1018 fear images\n",
      "797 surprise images\n",
      "1216 neutral images\n",
      "960 angry images\n",
      "111 disgust images\n"
     ]
    }
   ],
   "source": [
    "count_images_per_expression(base_path, subset='validation')\n",
    "#visualize_samples(base_path, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Error handling:---- don't run now\n",
    "for img_name in os.listdir(expression_path):\n",
    "    img_path = os.path.join(expression_path, img_name)\n",
    "    try:\n",
    "        img_array = preprocess_image(img_path)\n",
    "        images.append(img_array)\n",
    "        labels.append(label_map[expression])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "### Code Explanation:\n",
    "1. Preprocessing Function (preprocess_image):\n",
    "- preprocess_image function : to encapsulate the preprocessing steps for each image.\n",
    "- It loads an image from img_path, resizes it to (pic_size, pic_size) using load_img from Keras.\n",
    "- Converts the image to a numpy array (img_to_array).\n",
    "- Normalizes the pixel values by dividing by 255.0, ensuring all values are between 0 and 1.\n",
    "\n",
    "2. Integration:\n",
    "- Inside the nested loops, instead of loading and displaying raw images, we now call preprocess_image to obtain the preprocessed image array (img_array).\n",
    "- plt.imshow(img_array, cmap=\"gray\") displays the preprocessed image using Matplotlib.\n",
    "\n",
    "3. Visualization:\n",
    "- The code continues to iterate through each expression folder in the train directory, displaying up to 5 preprocessed images per expression.\n",
    "- Each image plot is labeled with its corresponding expression (plt.xlabel(expression)).\n",
    "\n",
    "* Lists for Storage: We've introduced two lists, images and labels, to store the preprocessed images and their corresponding labels.\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHALLENGES:\n",
    "- Identifying facial expressions from images is challenging for algorithms due to factors like :\n",
    "* the images have a low resolution\n",
    "* the faces are not in the same position\n",
    "* some images have text written on them\n",
    "* some people hide part of their faces with their hands\n",
    "- However all this diversity of images will contribute to make a more generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now the corrected o/p is:\n",
    "Train images shape: (28, 48, 48, 1)\n",
    "Train labels shape: (28,)\n",
    "Test images shape: (7, 48, 48, 1)\n",
    "Test labels shape: (7,)\n",
    "\n",
    "\n",
    "### Output Interpretation: \n",
    "\n",
    "1. Train images shape: (28, 48, 48, 1)\n",
    "- 28 samples for training\n",
    "- Each image is 48x48 pixels\n",
    "- Grayscale images with 1 channel (since they are loaded in grayscale)\n",
    "\n",
    "2. Train labels shape: (28,)\n",
    "- Corresponding labels for the training images\n",
    "\n",
    "3. Test images shape: (7, 48, 48, 1)\n",
    "- 7 samples for testing\n",
    "- Each image is 48x48 pixels\n",
    "- Grayscale images with 1 channel\n",
    "\n",
    "4. Test labels shape: (7,)\n",
    "- Corresponding labels for the testing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The image expressions in our training dataset are pretty balanced, except for the 'disgust' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (28821, 48, 48, 1)\n",
      "Training labels shape: (28821,)\n",
      "Test images shape: (7066, 48, 48, 1)\n",
      "Test labels shape: (7066,)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_dataset(base_path, subset='train')\n",
    "test_images, test_labels = load_dataset(base_path, subset='validation')\n",
    "\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
