{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "pic_size = 48                         \n",
    "\n",
    "\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/GitHub/Face-expression-recognition-with-Deep-Learning/images\"\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path, target_size=(48, 48)):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = load_img(img_path, target_size=target_size, color_mode=\"grayscale\")  # Grayscale for custom CNNs\n",
    "    img_array = img_to_array(img)  # Convert to NumPy array\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_dataset(base_path, subset='train'):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue  \n",
    "\n",
    "        for img_name in os.listdir(expression_path):\n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            try:\n",
    "                img_array = preprocess_image(img_path)\n",
    "                images.append(img_array)\n",
    "                labels.append(label_map[expression])  \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize sample images\n",
    "def visualize_samples(base_path, subset='train'):\n",
    "    \"\"\"Visualize sample images for each expression.\"\"\"\n",
    "    plt.figure(figsize=(12, 20))           \n",
    "    counter = 0                            \n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    \n",
    "    for expression in os.listdir(subset_path):                      \n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue                                                \n",
    "        \n",
    "        for i, img_name in enumerate(os.listdir(expression_path)[:5]):  \n",
    "            counter += 1                                               \n",
    "            plt.subplot(7, 5, counter)                                  \n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            img_array = preprocess_image(img_path)\n",
    "            plt.imshow(img_array, cmap=\"gray\")                         \n",
    "            plt.xlabel(expression)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to count images per expression\n",
    "def count_images_per_expression(base_path, subset='train'):\n",
    "    \"\"\"Count and display the number of images per expression.\"\"\"\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue\n",
    "        \n",
    "        num_images = len(os.listdir(expression_path))\n",
    "        print(f\"{num_images} {expression} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_per_expression(base_path, subset='train')\n",
    "##visualize_samples(base_path, subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_images_per_expression(base_path, subset='validation')\n",
    "#visualize_samples(base_path, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Error handling:---- don't run now\n",
    "#for img_name in os.listdir(expression_path):\n",
    "   # img_path = os.path.join(expression_path, img_name)\n",
    "   # try:\n",
    "    #    img_array = preprocess_image(img_path)\n",
    "     #   images.append(img_array)\n",
    "     #   labels.append(label_map[expression])\n",
    "   # except Exception as e:\n",
    "   #     print(f\"Error loading image {img_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48                                                       \n",
    "\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/GitHub/Face-expression-recognition-with-Deep-Learning/images\"\n",
    "\n",
    "batch_size = 64   \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,             \n",
    "    rotation_range=10,            \n",
    "    width_shift_range=0.1,        \n",
    "    height_shift_range=0.1,               \n",
    "    zoom_range=0.1,               \n",
    "    horizontal_flip=True,        \n",
    "    fill_mode='nearest'           \n",
    ")\n",
    "\n",
    "      \n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(base_path + \"/train\",           \n",
    "                                                    target_size=(pic_size,pic_size), \n",
    "                                                    color_mode=\"grayscale\",           \n",
    "                                                    batch_size=batch_size,            \n",
    "                                                    class_mode='categorical',         \n",
    "                                                    shuffle=True)                    \n",
    "validation_generator = validation_datagen.flow_from_directory(base_path + \"/validation\",\n",
    "                                                    target_size=(pic_size,pic_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dict = {0: 1.0311, 1: 9.4433, 2: 1.0035, 3: 0.5747, 4: 0.8264, 5: 0.8338, 6: 1.2846}\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate([\n",
    "    1.031125898894494, 9.443315858453474, 1.0034817729187702,\n",
    "    0.5747188322565207, 0.8264322991340254, 0.8337962159347335,\n",
    "    1.2846445286382884\n",
    "])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Number of labels :\n",
    "num_labels =  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "# 1st Convolution layer\n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "model.add(Dropout(0.25)) \n",
    "\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128, (5, 5), padding='same', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0001)                                                                 \n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "validation_steps = validation_generator.n // validation_generator.batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"model_weights.keras\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "callbacks=[checkpoint, early_stop]\n",
    "\n",
    "\n",
    "steps_per_epoch = train_generator.n // train_generator.batch_size\n",
    "validation_steps = validation_generator.n // validation_generator.batch_size\n",
    "print(f\"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}\")\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation samples: {validation_generator.n}\")\n",
    "print(f\"Batch size: {validation_generator.batch_size}\")\n",
    "print(f\"Steps per epoch (validation): {len(validation_generator)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer: Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix of our predictions\n",
    "\n",
    "# Compute predictions\n",
    "predictions = model.predict(validation_generator)\n",
    "y_pred = [np.argmax(probas) for probas in predictions]\n",
    "y_test = validation_generator.classes\n",
    "class_names = list(validation_generator.class_indices.keys())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('/Users/pratiksha/Documents/Pratiksha/Documents/GitHub/GitHub/Face-expression-recognition-with-Deep-Learning/my_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he 450/450 in the training log represents the number of steps (batches) that have been completed during the current epoch. It shows the following:\n",
    "\n",
    "450/450 means that the model has processed all 450 steps (batches) of the current epoch.\n",
    "The first 450 represents the number of steps that have been completed so far in the epoch.\n",
    "The second 450 is the total number of steps that the model will go through in this epoch (i.e., the number of batches).\n",
    "Since the model has reached 450/450, it means the epoch has finished, and all training data has been processed once.\n",
    "\n",
    "In short, it indicates the current progress of training in terms of the batches completed in the current epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Batch size** refers to the number of samples the model processes before updating its weights.\n",
    "- The **steps per epoch** refers to how many batches are needed to complete one full pass through the training dataset.\n",
    "\n",
    "So, in your case, the training generator is set to:\n",
    "\n",
    "- **Batch size = 64**: This means each batch contains 64 images.\n",
    "- **Steps per epoch = 450**: This means that in each epoch, the model will process 450 batches (each containing 64 images). This results in processing a total of \\( 450 \\times 64 \\) images in one epoch.\n",
    "\n",
    "If we calculate the total number of images processed during one epoch:\n",
    "\n",
    "\\[\n",
    "\\text{Total images per epoch} = 450 \\times 64 = 28,800 \\text{ images}\n",
    "\\]\n",
    "\n",
    "This means, in each epoch, the model will go through 28,800 images (divided into 450 batches of 64 images each).\n",
    "\n",
    "So, when you see `450/450`, it simply means that all 450 batches (of 64 images each) have been processed for that epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the value `450` for the **steps per epoch** is typically determined based on the size of your dataset and the batch size you are using.\n",
    "\n",
    "If you haven't explicitly set it, then the number `450` is calculated automatically based on the data generator and batch size.\n",
    "\n",
    "### How `steps_per_epoch` is calculated:\n",
    "It is determined as:\n",
    "\\[\n",
    "\\text{steps per epoch} = \\frac{\\text{total number of samples in training set}}{\\text{batch size}}\n",
    "\\]\n",
    "\n",
    "For instance:\n",
    "- If your training dataset has 28,800 images, and your batch size is 64, then:\n",
    "\\[\n",
    "\\text{steps per epoch} = \\frac{28,800}{64} = 450\n",
    "\\]\n",
    "\n",
    "So, `450` is not something you manually set, but it's the result of this calculation based on your data size and batch size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
