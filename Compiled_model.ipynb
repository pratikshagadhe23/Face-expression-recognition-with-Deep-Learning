{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48                            #Specifies the size of each image (48x48 pixels in this case)\n",
    "\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/GitHub/Face-expression-recognition-with-Deep-Learning/images\"\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path, target_size=(48, 48)):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = load_img(img_path, target_size=target_size, color_mode=\"grayscale\")  # Grayscale for custom CNNs\n",
    "    img_array = img_to_array(img)  # Convert to NumPy array\n",
    "    img_array /= 255.0  # Normalize pixel values\n",
    "    return img_array\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_dataset(base_path, subset='train'):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_map = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue  # Skip non-directory files\n",
    "\n",
    "        for img_name in os.listdir(expression_path):\n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            try:\n",
    "                img_array = preprocess_image(img_path)\n",
    "                images.append(img_array)\n",
    "                labels.append(label_map[expression])  # Use label_map safely\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Function to visualize sample images\n",
    "def visualize_samples(base_path, subset='train'):\n",
    "    \"\"\"Visualize sample images for each expression.\"\"\"\n",
    "    plt.figure(figsize=(12, 20))           # Specifies the size of the plot\n",
    "    counter = 0                            # Used to count the number of images displayed -- Counter variable\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    \n",
    "    for expression in os.listdir(subset_path):                      # Iterates through each expression folder (angry, disgust, etc.) within the train directory.\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue                                                # Skip if it's not a directory\n",
    "        \n",
    "        for i, img_name in enumerate(os.listdir(expression_path)[:5]):  # Loops through the first 5 images (i ranges from 1 to 5) within each expression folder.\n",
    "            counter += 1                                                # Increments the counter variable\n",
    "            plt.subplot(7, 5, counter)                                  # Creates a subplot grid with 7 rows and 5 columns, placing the current image in position cpt.\n",
    "            img_path = os.path.join(expression_path, img_name)\n",
    "            img_array = preprocess_image(img_path)\n",
    "            plt.imshow(img_array, cmap=\"gray\")                          # Displays the image with a grayscale colormap.\n",
    "            plt.xlabel(expression)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to count images per expression\n",
    "def count_images_per_expression(base_path, subset='train'):\n",
    "    \"\"\"Count and display the number of images per expression.\"\"\"\n",
    "    subset_path = os.path.join(base_path, subset)\n",
    "    for expression in os.listdir(subset_path):\n",
    "        expression_path = os.path.join(subset_path, expression)\n",
    "        if not os.path.isdir(expression_path):\n",
    "            continue\n",
    "        \n",
    "        num_images = len(os.listdir(expression_path))\n",
    "        print(f\"{num_images} {expression} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164 happy images\n",
      "4938 sad images\n",
      "4103 fear images\n",
      "3205 surprise images\n",
      "4982 neutral images\n",
      "3993 angry images\n",
      "436 disgust images\n"
     ]
    }
   ],
   "source": [
    "count_images_per_expression(base_path, subset='train')\n",
    "##visualize_samples(base_path, subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1825 happy images\n",
      "1139 sad images\n",
      "1018 fear images\n",
      "797 surprise images\n",
      "1216 neutral images\n",
      "960 angry images\n",
      "111 disgust images\n"
     ]
    }
   ],
   "source": [
    "count_images_per_expression(base_path, subset='validation')\n",
    "#visualize_samples(base_path, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Error handling:---- don't run now\n",
    "#for img_name in os.listdir(expression_path):\n",
    "   # img_path = os.path.join(expression_path, img_name)\n",
    "   # try:\n",
    "    #    img_array = preprocess_image(img_path)\n",
    "     #   images.append(img_array)\n",
    "     #   labels.append(label_map[expression])\n",
    "   # except Exception as e:\n",
    "   #     print(f\"Error loading image {img_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (28821, 48, 48, 1)\n",
      "Training labels shape: (28821,)\n",
      "Test images shape: (7066, 48, 48, 1)\n",
      "Test labels shape: (7066,)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_dataset(base_path, subset='train')\n",
    "test_images, test_labels = load_dataset(base_path, subset='validation')\n",
    "\n",
    "print(\"Training images shape:\", train_images.shape)\n",
    "print(\"Training labels shape:\", train_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Size of the image: 48x48 pixels\n",
    "pic_size = 48                                                       #Specifies the size of each image (48x48 pixels in this case)\n",
    "\n",
    "\n",
    "# Input path for the images\n",
    "base_path = \"/Users/pratiksha/Documents/Pratiksha/GitHub/Face-expression-recognition-with-Deep-Learning/images\"\n",
    "\n",
    "# number of images to feed into the NN for every batch\n",
    "batch_size = 16     #Specifies the number of images to feed into the neural network for every batch during training and validation.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,              # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,            # Randomly rotate images in the range (degrees)\n",
    "    width_shift_range=0.2,        # Randomly translate images horizontally\n",
    "    height_shift_range=0.2,       # Randomly translate images vertically\n",
    "    shear_range=0.2,              # Apply shearing transformations\n",
    "    zoom_range=0.2,               # Randomly zoom images\n",
    "    horizontal_flip=True,         # Randomly flip images horizontally\n",
    "    fill_mode='nearest'           # Filling method for points outside boundaries\n",
    ")\n",
    "\n",
    "                            #ImageDataGenerator : it is used to generate a batch of images with some random transformations\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(base_path + \"/train\",            # flow_from_directory: Generates batches of augmented/normalized data from image files in the train directory.\n",
    "                                                    target_size=(pic_size,pic_size), # Resizes images to (pic_size, pic_size) pixels.\n",
    "                                                    color_mode=\"grayscale\",           # Converts images to grayscale format.\n",
    "                                                    batch_size=batch_size,            # Number of images per batch to be yielded from the generator.\n",
    "                                                    class_mode='categorical',         # Returns one-hot encoded labels for multi-class classification.\n",
    "                                                    shuffle=True)                     #  Shuffles the order of images after every epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(base_path + \"/validation\", # fetches batches of validation data from the validation directory.\n",
    "                                                    target_size=(pic_size,pic_size),\n",
    "                                                    color_mode=\"grayscale\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for the training data\n",
    "classes = train_generator.classes  # Extract class indices from the train generator\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(\"Computed class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Number of labels :\n",
    "num_labels =  7\n",
    "\n",
    " # Creating a sequential model :\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Convolution layer\n",
    "model.add(Conv2D(36,(3,3), padding='same', input_shape=(48, 48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))  #read max pooling adv gpt****************************************\n",
    "model.add(Dropout(0.25)) #add the range table for dropout\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer 1st layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu')) # This is sufficient to prepare the output for dense layers.\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # 7 output units for 7 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0001)                                                                 #optimizer: Specifies the optimizer (Adam in this case) to use for training.\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])   #metrics: Specifies the metrics to evaluate the model (accuracy in this case).\n",
    "     #loss: Specifies the loss function (categorical crossentropy for multi-class classification).\n",
    "     #categorical_crossentropy : Computes the cross-entropy loss between true labels and predicted labels.\n",
    "     #crossentropy : measures the performance of a classification model whose output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "# Specifies the number of epochs to train the model. Each epoch means one complete pass through the entire training dataset.\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# Imports the ModelCheckpoint class from keras.callbacks. This is used to save the model at certain points during training.\n",
    "\n",
    "# Define the checkpoint to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"model_weights.keras\",  # The file path where the model weights will be saved. Updated to end with .keras for clarity.\n",
    "    monitor='val_accuracy',  # Metric to be monitored. 'val_accuracy' means validation accuracy.\n",
    "    verbose=1,  # Verbosity mode. 1 means that messages will be printed when the model is being saved.\n",
    "    save_best_only=True,  # If True, the latest best model according to the monitored metric will not be overwritten.\n",
    "    mode='max'  # Mode for the monitored metric. 'max' means that the model will be saved when the quantity monitored has stopped increasing.\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "# A list of callbacks to pass to the model during training. Here, it contains only the checkpoint callback.\n",
    "\n",
    "# Train the model using model.fit instead of model.fit_generator\n",
    "history = model.fit(\n",
    "    x=train_generator,  # The training data. Here, train_generator is used to provide batches of data during training.\n",
    "    epochs=epochs,  # The number of epochs to train the model.\n",
    "    steps_per_epoch=train_generator.n // train_generator.batch_size,  \n",
    "    # The number of steps per epoch. This is the total number of samples in the training data divided by the batch size.\n",
    "    \n",
    "    validation_data=validation_generator,  # The validation data. Here, validation_generator is used to provide batches of validation data.\n",
    "    validation_steps=validation_generator.n // validation_generator.batch_size,  \n",
    "    # The number of validation steps per epoch. This is the total number of samples in the validation data divided by the batch size.\n",
    "    \n",
    "    callbacks=callbacks_list  # List of callbacks to apply during training. Here, it includes the checkpoint callback to save the best model.\n",
    ")\n",
    "\n",
    "# Explanation for validation_generator.n // validation_generator.batch_size:\n",
    "# This calculates the number of batches (steps) needed to go through the entire validation dataset once.\n",
    "# - validation_generator.n is the total number of samples in the validation dataset.\n",
    "# - validation_generator.batch_size is the number of samples in each batch.\n",
    "# The integer division (//) ensures we get the whole number of batches. For example, if there are 1000 validation samples and the batch size is 32,\n",
    "# this would be 1000 // 32 = 31 steps per epoch for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle('Optimizer: Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix of our predictions\n",
    "\n",
    "# Compute predictions\n",
    "predictions = model.predict(validation_generator)\n",
    "y_pred = [np.argmax(probas) for probas in predictions]\n",
    "y_test = validation_generator.classes\n",
    "class_names = list(validation_generator.class_indices.keys())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('/Users/pratiksha/Documents/Pratiksha/Documents/GitHub/GitHub/Face-expression-recognition-with-Deep-Learning/my_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
